{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensayo Support Vector Machine (SVM) \n",
    "## Álvaro Andrés Esquivel Gómez 11002822"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hipótesis de SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support vector machine (SVM), es un algotimo de aprendizaje supervisado para la resulución de problemas de clasificación de datos o regresión. \n",
    "* Margen: es la ditancia mas corta que hay entre las observaciones y el umbral. SVM busca que los margenes sea iguales para cada tipo diferente de observación. \n",
    "* SVM permite que el umbral tencla errores en la clasificacion de las clases, con la finalidad de no ser tan sensible a valores atípicos. Esto perrmite una mejor represerntación de la tendencia de los datos, y disminuye el sobreajuste que acusaria demasiado error contra los datos de validación. \n",
    "* Cuando se permiten clasificaciones erroneas de las observaciones durante el entrenamiento se conoce como: Soft Margin.\n",
    "* Utilizando Cross Validation, podemos determinar el mejor Soft Margin para la clasificación de los datos.\n",
    "\n",
    "La siguiente imagen ilustra como se visualiza el Soft Margin usando Cros Validation para la clasificación de dos tipos de datos diferentes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/svm1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que se permiten algunos errores en la clasificación con la finalidad de no tener un sobreajuste alto que afecte el modelo. En el ejemplo, una observacion roja esta en el Soft Margin clasificada como verde.\n",
    "\n",
    "Para entender las formulas de SVM, en la siguiente imagen vemos como las lineas marginales estan divididas por un espacio, donde $$\\vec{w}$$ es el vector perpendicular del hiperplano, y el parametro: $$\\tfrac{b}{\\|\\vec{w}\\|}$$ determina el desplazamiento del hiperplano desde el origen a lo largo del vector w.\n",
    "\n",
    "De esta manera obtenemos las ecuaciones de hiperplano y las ecuaciones de los margenes. \n",
    "\n",
    "$$\\vec{w} \\cdot \\vec{x} - b = 1$$\n",
    "$$\\vec{w} \\cdot \\vec{x} - b = 0$$\n",
    "$$\\vec{w} \\cdot \\vec{x} - b = -1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/SVM_margin.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por tanto, podemos definir la funcion de prediccion o hipotesis para el ejemplo anterior como:\n",
    "$$\\hat y = \\begin{cases} 0 & \\text{si } \\vec{w} \\cdot \\vec{x} - b < 0 \\\\ 1 & \\text{si } \\vec{w} \\cdot \\vec{x} - b >= 0  \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de costo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para encontrar la funcion de costo se debe determinar la separación o margen entre la ecuación central. Se debe de optimizar: $$\\tfrac{2}{\\|\\vec{w}\\|} \\text{  o lo que es igual a,  } \\frac{1}{2}\\lVert \\vec{w} \\rVert^2 $$ \n",
    "\n",
    "Al introducir la función de costo hinge en la ecuación: \n",
    "$$\\max\\left(0, 1 - y_i(\\vec{w} \\cdot \\vec{x}_i - b)\\right)$$\n",
    "\n",
    "Adicionando el parametro lambda para aumentar el margen y controlar el sobreajuste. Obtenemos la siguiente ecuación:\n",
    "\n",
    "$$\\frac{1}{2}\\lVert \\vec{w} \\rVert^2 + \\lambda\\left[\\frac 1 n \\sum_{i=1}^n \\max\\left(0, 1 - y_i(\\vec{w} \\cdot \\vec{x}_i - b)\\right) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A la ecuación de coste se debe de incluir la regularización L2, para ello, se realiza la sumatoria de los parámetros entrenables al cuadrado.\n",
    "Para este caso se considera a la variable C como el opuesto de lambda, por tanto un valor grande de C puede provocar un sobreajuste. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/costoSVM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmo de aprendizaje/entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entrenar el algotimo, podemos escribir la ecuación anterior como: \n",
    "\n",
    "$$y (\\vec{w} \\cdot \\vec{x} + b) = z$$\n",
    "\n",
    "Al aplicar valores en la ecuación, podemos esperar un comportamiento similar a la grafica siguiente:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![title](img/entrenamientoSVM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando el valor de z es mayor a uno, la clasificación es correcta, según la recta de hinge. Sin embargo, cuando un z es  un numero menor a 1, la clasificación es incorrecta. \n",
    "\n",
    "A medida que el valor de z, es mucho menor a 1, la clasificación se considera incorrecta en mayor medida. El comportamiento esta dado por:\n",
    "\n",
    "$$ z = \\begin{cases} 0 & \\text{si } z  >= 1 \\\\ 1 - z & \\text{si } z < 1  \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propiedades, similitudes, diferencias ,ventajas y desventajas sobre otros algoritmos y modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas propiedades de SVM con respecto a otros algoritmos puede ser:\n",
    "\n",
    "#### Propiedades, similitudes y diferencias\n",
    "* La principal diferencia con respecto a otros modelos, es que SVM da una margen de clasificación, es decir, no solo busca una linea que separe a las clasificaciones, sino adicionalmente, espera que exista un magen entre ellos, permitiendo algunos errores en la clasificación para evitar un sobreajuste.\n",
    "* Radial Kernel, puede comportarse de forma similar a KNN.\n",
    "* Se debe encontrar la optimización el la función de costo para encontrar los mejores valores.\n",
    "\n",
    "#### Ventajas\n",
    "* SVM perminte el manejo de multiples dimenciones de features.\n",
    "* El algoritmo de SVM tiende a ser de rapido entrenamiento, y puede ser muy útil, sobre todo para dataset pequeños.\n",
    "* Eficiente en el uso de la memoria. \n",
    "\n",
    "#### Desventajas\n",
    "* No es posible una clasificación direceta entre multiples clases, se debe de implementar algoritmos del tipo one-vs-others\n",
    "* Siempre se debe hacer feature engineering para proveer al algoritmo de datos de calidad.\n",
    "* No puede predecir valores con probabilidades, únicamente valores categoricos.\n",
    "* Su performance no es tan bueno cuando se tienen demasiados datos. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel-trick/basis functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM utiliza las funciones Kernel con la finalidad de encontrar una clasificacion adecuada cuando se tienen varias dimenciones de features, de forma que no se pueden separar linealmente. \n",
    "* Los Kernels miden la similitud entre dos vectores o pares de puntos. Esto lo realiza mediante la tranformación de features.\n",
    "\n",
    "Por lo tanto, es calculado mediante la distancia euclidiana de dos vectores, tomando el parametro σ como la suavidad de la función. Si tomamos a x ≈ l⁽¹⁾, f1 ≈ 1, si x esta muy alejado de l⁽¹⁾, entonces f1 ≈ 0.\n",
    "\n",
    "Lo cual se puede expresar de la siguiente manera:\n",
    "\n",
    "\n",
    "![title](img/kernelsSVM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la fórmula anterior, lo que hacemos es encontrar la similitud entre nuevos datos y cada feature. El resultado de cada f es una nueva dimensión que representa a la similitud estimada. \n",
    "\n",
    "En SVM al transformar los datos de entrenamiento, estas variables por tanto, se encuentran en un espacio lantente o inferido. \n",
    "\n",
    "Sin embargo, esta no es la única ecuación Kernel, existen otras también muy utilizadas como las siguientes:\n",
    "\n",
    "![title](img/kernelSVM3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "* SVM es un algoritmo muy útil para la clasificación de tipos de datos, que puede ser utilizado sin importar la dimencionalidad de las features.\n",
    "\n",
    "* SVM es una excelente opción para dataset pequeños, como los que se encuentran en la realidad en muchas ocaciones.\n",
    "\n",
    "* Una importante opción para agregar entre nuestras herramientas de aprendizaje supervisado. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "\n",
    "* Support Vector Machines, Clearly Explained -- https://www.youtube.com/watch?v=efR1C6CvhmE\n",
    "* Support vector machine, Wikipedia -- https://en.wikipedia.org/wiki/Support_vector_machine\n",
    "* Loss Function(Part III): Support Vector Machine, Shuyu Luo -- https://towardsdatascience.com/optimization-loss-function-under-the-hood-part-iii-5dff33fa015d\n",
    "* A Complete Guide To Support Vector Machines(SVMs), KUSHAL CHAKRABORTY - https://medium.com/@kushaldps1996/a-complete-guide-to-support-vector-machines-svms-501e71aec19e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
